{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4020a96",
   "metadata": {},
   "source": [
    "### Gridworld value evaluation\n",
    "\n",
    "We consider a 5x5 grid as the environment and each position in the grid (row, column) defines a different state. An agent is allowed to move one block in any if the four directions (north, south, east, west). A (0,1) and B(0,3) are special states, ie, if the agent reaches A, it will get a reward of +10 and transport directly to A'(4,1) and if it reaches B, it will get a reward of +5 and transport directly to B'(2,3).\n",
    "If the agent tries to move off the edge, it will get a reward of -1, but will continue to stay within the grid.\n",
    "The agent is allowed to choose any direction with uniform probability. i.e, a uniform random policy is used.\n",
    "\n",
    "The goal is to compute the state-valye function for all states under the given policy, which tells us the expected return (future cumulative discounted reward) starting from each state.\n",
    "We use iterative policy evaluation, applying the Bellman expectation equation repeatedly until the values converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "290a1bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efb66634",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 5\n",
    "states = [(i,i) for i in range(grid_size) for j in range(grid_size)]\n",
    "actions = [(-1,0),(1,0),(0,-1),(0,1)]\n",
    "actions_prob = 0.25\n",
    "gamma = 0.9\n",
    "theta = 0.0001\n",
    "\n",
    "specials = {\n",
    "    (0,1): ((4,1),10),\n",
    "    (0,3):((2,3),5)\n",
    "}\n",
    "\n",
    "v_pi = np.zeros((grid_size, grid_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7718e141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special state: (4, 1), reward: 10\n"
     ]
    }
   ],
   "source": [
    "next_state, reward = specials[(0,1)]\n",
    "print(f\"Special state: {next_state}, reward: {reward}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6079b331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f56a9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(state,action):\n",
    "    \"\"\"\n",
    "    Takes state and action as input\n",
    "    returns the next state and reward\n",
    "    reward = +10 if state is (0,10) and +5 if state is (0,3)\n",
    "    reward = -1 if action would lead to out of bounds\n",
    "    reward = 0 otherwise\n",
    "    \"\"\"\n",
    "    if state in specials:\n",
    "        state_next, reward = specials[state]\n",
    "        return state_next, reward\n",
    "    i, j = state\n",
    "    di, dj = action\n",
    "    ni, nj = i + di, j + dj\n",
    "\n",
    "    if 0<= ni < grid_size and 0 <= nj < grid_size:\n",
    "        return (ni,nj), 0\n",
    "    else:\n",
    "        return state,-1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d93518dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function after 1 iterations:\n",
      "[[ 3.31  8.79  4.43  5.32  1.49]\n",
      " [ 1.52  2.99  2.25  1.91  0.55]\n",
      " [ 0.05  0.74  0.67  0.36 -0.4 ]\n",
      " [-0.97 -0.43 -0.35 -0.58 -1.18]\n",
      " [-1.86 -1.34 -1.23 -1.42 -1.97]]\n"
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "while True:\n",
    "    delta = 0\n",
    "    new_v = np.zeros_like(v_pi)\n",
    "\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            s = (i, j)\n",
    "            value = 0\n",
    "            for a in actions:\n",
    "                next_s, r = step(s, a)\n",
    "                ni, nj = next_s\n",
    "                value += actions_prob * (r + gamma * v_pi[ni, nj])\n",
    "            new_v[i, j] = value\n",
    "            delta = max(delta, abs(v_pi[i, j] - value))\n",
    "\n",
    "    v_pi[:] = new_v\n",
    "    iteration += 1\n",
    "    if delta < theta:\n",
    "        break\n",
    "\n",
    "# Print final value function\n",
    "print(\"Value function after\", iteration, \"iterations:\")\n",
    "print(np.round(v_pi, 2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
